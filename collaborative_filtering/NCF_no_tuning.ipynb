{"cells":[{"cell_type":"markdown","metadata":{"id":"BaxCmFL1iq_O"},"source":["## Neural Collaborative Filtering"]},{"cell_type":"markdown","metadata":{"id":"YjXVT2U-Pf1t"},"source":["### Importing Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"60ssvIbjPlpu"},"outputs":[{"name":"stdout","output_type":"stream","text":["System version: 3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]\n","Pandas version: 1.4.4\n"]}],"source":["import sys\n","import os\n","import shutil\n","\n","# Pandas and Numpy is used for efficient handling of arrays.\n","import pandas as pd\n","import numpy as np\n","\n","\n","from recommenders.utils.timer import Timer\n","from recommenders.datasets.python_splitters import python_chrono_split\n","\n","# importing the dataset\n","from recommenders.datasets import movielens\n","from recommenders.models.ncf.dataset import Dataset as NCFDataset\n","\n","# Importing the NCF model class from the recommenders library\n","from recommenders.models.ncf.ncf_singlenode import NCF\n","\n","# importing the evaluation metrics\n","from recommenders.evaluation.python_evaluation import (rmse, mae, rsquared, exp_var, map_at_k, ndcg_at_k, precision_at_k,\n","                                                     recall_at_k, get_top_k_items)\n","from recommenders.utils.constants import SEED as DEFAULT_SEED\n","\n","\n","print(\"System version: {}\".format(sys.version))\n","print(\"Pandas version: {}\".format(pd.__version__))"]},{"cell_type":"markdown","metadata":{"id":"LueGX40FQLIU"},"source":["### Loading the Dataset"]},{"cell_type":"markdown","metadata":{"id":"A5mQ-eiNCeh8"},"source":["We will be using the movielens dataset. It contains the user, movie and the rating given by the user."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>userID</th>\n","      <th>itemID</th>\n","      <th>rating</th>\n","      <th>timestamp</th>\n","      <th>song</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>577</td>\n","      <td>30377</td>\n","      <td>0.172815</td>\n","      <td>1.447978e+09</td>\n","      <td>The Safety Dance by Men Without Hats</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>147</td>\n","      <td>15910</td>\n","      <td>0.127811</td>\n","      <td>1.441325e+09</td>\n","      <td>Endless Summer by Grizfolk</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>690</td>\n","      <td>40737</td>\n","      <td>0.097224</td>\n","      <td>1.413331e+09</td>\n","      <td>Castaway by Zac Brown Band</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>59</td>\n","      <td>40737</td>\n","      <td>0.103762</td>\n","      <td>1.404950e+09</td>\n","      <td>Castaway by Zac Brown Band</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>415</td>\n","      <td>49732</td>\n","      <td>0.102779</td>\n","      <td>1.402963e+09</td>\n","      <td>Islands In the Stream by Dolly Parton</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   userID  itemID    rating     timestamp  \\\n","0     577   30377  0.172815  1.447978e+09   \n","1     147   15910  0.127811  1.441325e+09   \n","2     690   40737  0.097224  1.413331e+09   \n","3      59   40737  0.103762  1.404950e+09   \n","4     415   49732  0.102779  1.402963e+09   \n","\n","                                    song  \n","0   The Safety Dance by Men Without Hats  \n","1             Endless Summer by Grizfolk  \n","2             Castaway by Zac Brown Band  \n","3             Castaway by Zac Brown Band  \n","4  Islands In the Stream by Dolly Parton  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["#TODO Load the CSV file into a dataframe\n","df = pd.read_csv('cf_final2.csv')\n","df= df.rename(columns={\"user\":\"userID\", 'item':\"itemID\", \"label\":\"rating\", \"song_by\": \"song\"})\n","df=df[[\"userID\",\"itemID\",\"rating\",\"timestamp\",\"song\"]]\n","df.head(5)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"OAEWJQHwPoqD"},"outputs":[],"source":["# top k items to recommend\n","TOP_K = 10\n","\n","# Model parameters\n","# Number of iterations during the training process\n","EPOCHS = 25\n","# Batch size means how many user-item pairs you want to predict at once\n","BATCH_SIZE = 256\n","\n","# Setting seed to remove any stochasticity and reproduce results\n","SEED = DEFAULT_SEED  # Set N"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"EmnOf4Hyqde3"},"outputs":[],"source":["# Splitting the dataset.\n","# 75% will be used during training and 25% will be used during testing\n","\n","train, test = python_chrono_split(df, 0.75)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"BM5p4kY8qjco"},"outputs":[],"source":["# Filtering out users and items in the test set that do not appear in the training set.\n","# This is done so that we can see if our model has learnt user's previous item interactions and can recommend relevant items.\n","\n","test = test[test[\"userID\"].isin(train[\"userID\"].unique())]\n","test = test[test[\"itemID\"].isin(train[\"itemID\"].unique())]\n","\n","# Creating a test set which only contains the last interaction for each user. Remaining data of the user is used in the train set\n","leave_one_out_test = test.groupby(\"userID\").last().reset_index()\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"3HOAfqPzY5sx"},"outputs":[],"source":["# Writing the data into csv files\n","\n","train_file = \"./train.csv\"\n","test_file = \"./test.csv\"\n","leave_one_out_test_file = \"./leave_one_out_test.csv\"\n","train.to_csv(train_file, index=False)\n","test.to_csv(test_file, index=False)\n","leave_one_out_test.to_csv(leave_one_out_test_file, index=False)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9124,"status":"ok","timestamp":1691498149527,"user":{"displayName":"Harsh","userId":"03191827286572319654"},"user_tz":-480},"id":"EkQXPSwYY83Q","outputId":"830ec558-0b7a-417c-e8bb-af872f4dc81e"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:recommenders.models.ncf.dataset:Indexing ./train.csv ...\n","INFO:recommenders.models.ncf.dataset:Indexing ./leave_one_out_test.csv ...\n","INFO:recommenders.models.ncf.dataset:Creating full leave-one-out test file ./leave_one_out_test_full.csv ...\n","100%|██████████| 717/717 [00:06<00:00, 104.28it/s]\n","INFO:recommenders.models.ncf.dataset:Indexing ./leave_one_out_test_full.csv ...\n"]}],"source":["data = NCFDataset(train_file=train_file, test_file=leave_one_out_test_file, seed=SEED, overwrite_test_file_full=True)"]},{"cell_type":"markdown","metadata":{"id":"5priPx_BSYWN"},"source":["### Training the NCF Model"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7372,"status":"ok","timestamp":1691498156884,"user":{"displayName":"Harsh","userId":"03191827286572319654"},"user_tz":-480},"id":"5cTcHa8OZBwu","outputId":"a7eaed80-4e8a-4084-b887-93f37dbba231"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\darvi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  warnings.warn('`layer.apply` is deprecated and '\n"]}],"source":["model = NCF (\n","    n_users=data.n_users,\n","    n_items=data.n_items,\n","    model_type=\"NeuMF\",\n","    n_factors=4,\n","    layer_sizes=[16,8,4],\n","    n_epochs=EPOCHS,\n","    batch_size=BATCH_SIZE,\n","    learning_rate=1e-3,\n","    verbose=10,\n","    seed=SEED\n",")"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":262702,"status":"ok","timestamp":1691498419575,"user":{"displayName":"Harsh","userId":"03191827286572319654"},"user_tz":-480},"id":"pfMjgAjgZFeQ","outputId":"d6997f02-5caf-4be3-ee18-1f4dcf73ce29"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:recommenders.models.ncf.ncf_singlenode:Epoch 10 [11.02s]: train_loss = 0.173687 \n","INFO:recommenders.models.ncf.ncf_singlenode:Epoch 20 [11.10s]: train_loss = 0.096262 \n"]},{"name":"stdout","output_type":"stream","text":["Took 279.8980634 seconds for training.\n"]}],"source":["# Fitting the model on the training data. This can take 4 to 17 mins. Depending on n_epochs and if you are running on CPU/GPU.\n","\n","with Timer() as train_time:\n","    model.fit(data)\n","\n","print(\"Took {} seconds for training.\".format(train_time.interval))"]},{"cell_type":"markdown","metadata":{"id":"PtU6-V-7Ym-Z"},"source":["### Prediction and Evaluation\n","\n","Getting predictions from our trained model. We are converting it to a pandas dataframe later."]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":37243,"status":"ok","timestamp":1691498456805,"user":{"displayName":"Harsh","userId":"03191827286572319654"},"user_tz":-480},"id":"4_ab5_n2rWs6","outputId":"b9b91bea-810c-4fd0-e070-a05b225cb424"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>userID</th>\n","      <th>itemID</th>\n","      <th>prediction</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>26234</td>\n","      <td>0.000007</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>50800</td>\n","      <td>0.997452</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>16571</td>\n","      <td>0.028526</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>17377</td>\n","      <td>0.859025</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2</td>\n","      <td>8182</td>\n","      <td>0.196734</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   userID  itemID  prediction\n","0       0   26234    0.000007\n","1       1   50800    0.997452\n","2       1   16571    0.028526\n","3       1   17377    0.859025\n","4       2    8182    0.196734"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["predictions = [[row.userID, row.itemID, model.predict(row.userID, row.itemID)]\n","               for (_, row) in test.iterrows()]\n","\n","\n","predictions = pd.DataFrame(predictions, columns=['userID', 'itemID', 'prediction'])\n","predictions.head()"]},{"cell_type":"markdown","metadata":{"id":"hPTU8a8mGtFR"},"source":["In this step we are removing items that have already been rated by the user. We do not want to recommend the same item again to the user."]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16702,"status":"ok","timestamp":1691498473505,"user":{"displayName":"Harsh","userId":"03191827286572319654"},"user_tz":-480},"id":"7sr3cfuysUAC","outputId":"8f7e3928-c6f6-4653-8c29-046e8382e33e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Took 83.99911450000002 seconds for prediction.\n"]}],"source":["with Timer() as test_time:\n","\n","    users, items, preds = [], [], []\n","    item = list(train.itemID.unique())\n","    for user in train.userID.unique():\n","        user = [user] * len(item)\n","        users.extend(user)\n","        items.extend(item)\n","        preds.extend(list(model.predict(user, item, is_list=True)))\n","\n","    all_predictions = pd.DataFrame(data={\"userID\": users, \"itemID\":items, \"prediction\":preds})\n","\n","    merged = pd.merge(train, all_predictions, on=[\"userID\", \"itemID\"], how=\"outer\")\n","    all_predictions = merged[merged.rating.isnull()].drop('rating', axis=1)\n","\n","print(\"Took {} seconds for prediction.\".format(test_time.interval))"]},{"cell_type":"markdown","metadata":{"id":"gYTotvpnYsLX"},"source":["#### MAP\n","\n","It is the average precision for each user normalized over all users."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2052,"status":"ok","timestamp":1691498475547,"user":{"displayName":"Harsh","userId":"03191827286572319654"},"user_tz":-480},"id":"opVrJV2fsZia","outputId":"b58c6724-055e-4cb3-a335-ab57bf5b3c4c"},"outputs":[{"name":"stdout","output_type":"stream","text":["MAP: 0.000952636391966936\n"]}],"source":["eval_map = map_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n","print(f\"MAP: {eval_map}\")"]},{"cell_type":"markdown","metadata":{"id":"WE43tMC4YwXJ"},"source":["#### NDCG\n","\n","Normalized Discounted Cumulative Gain (NDCG) - evaluates how well the predicted items for a user are ranked based on relevance\n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3184,"status":"ok","timestamp":1691498478729,"user":{"displayName":"Harsh","userId":"03191827286572319654"},"user_tz":-480},"id":"LWJZI93jn6jI","outputId":"4dc80c5f-44b2-4e3f-e38f-4f8c86edb410"},"outputs":[{"name":"stdout","output_type":"stream","text":["NDCG: 0.0037502676510127297\n"]}],"source":["eval_ndcg = ndcg_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n","print(f\"NDCG: {eval_ndcg}\")"]},{"cell_type":"markdown","metadata":{"id":"lMECdyAPYyV2"},"source":["#### Precision Recall\n","\n","Precision - this measures the proportion of recommended items that are relevant\n","\n","Recall - this measures the proportion of relevant items that are recommended"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8205,"status":"ok","timestamp":1691498486923,"user":{"displayName":"Harsh","userId":"03191827286572319654"},"user_tz":-480},"id":"_FWpwaikn-6v","outputId":"63a433c4-e763-4637-9cad-264ca30107d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Precision: 0.003626220362622037 \n"," Recall: 0.0034648038309126185\n"]}],"source":["eval_precision = precision_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n","eval_recall = recall_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n","print(f\"Precision: {eval_precision} \\n Recall: {eval_recall}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1Y8KsX_hx0U6FOQO_cVNg90S4GCQBCW9g","timestamp":1691995771339}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
